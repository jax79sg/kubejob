{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform Advanced Developers Tutorial 1\n",
    "### Training Deep Learning Algorithms using Kubernetes on the AI Platform <br> By Jax and Just Another Deep Learning Enthusiast (JADLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Article\n",
    "\n",
    "This article builds on top of the initial <a href=https://github.com/jax79sg/kubejob/blob/master/single-train/README.md>article</a> by Jax. In his article, the basics of building a Docker image, pushing it to a Docker repository and running the training algorithm in Kubernetes is discussed. In this article, we will be training a chatbot using <a href=https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>movie dialogue</a> data. The initial steps of building the Docker image and sending the job to Kubernetes are largely similar but in this case, it is more targeted towards the AI platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Our Docker Image\n",
    "\n",
    "<a href=https://kubernetes.io/>Kubernetes</a> is an orchestration tool which automates management of containerized applications. Before we discuss how to perform training of your Deep Learning algorithm using Kubernetes, your <a href=https://www.docker.com/>Docker</a> image needs to be uploaded into the Docker repository in the AI platform. This requires you to package all required software libraries and tools into a <a href=https://docs.docker.com/engine/reference/commandline/build/>Docker image</a> in an Internet-enabled machine with Docker installed built. \n",
    "```bash\n",
    "docker build -t my_tensorflow:2.2.0-gpu-dialogue .\n",
    "```\n",
    "In this example, our <code>dockerfile</code> is relatively straightforward. _Since the docker image that we pull is running as the root user, we do not need to specify_ <code>sudo</code> _in our dockerfile_.\n",
    "```dockerfile\n",
    "FROM tensorflow/tensorflow:2.2.0-gpu\n",
    "RUN apt-get update && apt-get -y upgrade\n",
    "RUN pip install nltk boto3 numpy pandas pickle\n",
    "COPY deep_learning_train.py /home/deep_learning_train.py\n",
    "WORKDIR /home/\n",
    "```\n",
    "\n",
    "Check that your Docker image has been successfully built using <code>docker image list</code>. Assuming that the build was successful, you should see your docker image in the list of Docker images available locally. After your image is built, save via:\n",
    "```bash\n",
    "docker save --output my_docker_image.tar my_tensorflow:2.2.0-gpu-dialogue\n",
    "```\n",
    "The extracted docker image <code>my_docker_image.tar</code> will be saved in the path where you ran the command. Now, you can transfer your Docker image to the client machine before uploading it to Docker repository in the AI platform. In the client machine, run the following commands:\n",
    "```bash\n",
    "docker load my_docker_image.tar my_tensorflow:2.2.0-gpu-dialogue\n",
    "docker tag my_tensorflow:2.2.0-gpu-dialogue dockrepo.dh.gov.sg/my_tensorflow:2.2.0-gpu-dialogue\n",
    "docker push dockrepo.dh.gov.sg/my_tensorflow:2.2.0-gpu-dialogue\n",
    "```\n",
    "Once the Docker image is pushed into the Docker repository of the AI platform, we can proceed to use Kubernetes to train our model using the AI platform's computing resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Our Image in Kubernetes\n",
    "\n",
    "Before calling Kubernetes, we first need to define a .yaml file. This file specifies key information for Kubernetes, including which image to pull, as well as the optimal amount of resources which you require while training your model. Do note that the pod will not run if Kubernetes is unable to assign the required resources specified, so please refrain from requesting large amount of resources. \n",
    "\n",
    "Let us now look at how to define a <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/>.yaml</a> file. We remark that the number of spacing between the different levels is not important, as long as it is _consistent_.\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: dialogue-train\n",
    "  labels:\n",
    "    purpose: dialogue-train\n",
    "spec:\n",
    "  containers:\n",
    "  - name: dialogue-train-container\n",
    "    image: dockrepo.dh.gov.sg/my_tensorflow:2.2.0-gpu-dialogue\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"8\"\n",
    "        memory: \"64Gi\"\n",
    "        nvidia.com/gpu: \"1\"\n",
    "      requests:\n",
    "        cpu: \"4\"\n",
    "        memory: \"48Gi\"\n",
    "        nvidia.com/gpu: \"1\"\n",
    "    command: [\"python\", \"/home/dialogue_trial_transformer_tf_ver2.py\"]\n",
    "    args: [\"-b\", \"128\", \"-d\", \"100\", \"-n\", \"500000\"]\n",
    "  restartPolicy: Never\n",
    "```\n",
    "\n",
    "We bring to your attention the following parameters. Under <code>spec</code>, the image specifies the Docker image to pull from the repository, while the <code>resources</code> indicate how much compute resources your code is requesting. One interesting thing to note is that Kubernetes is able to <a href=https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/>overwrite</a> the underlying Docker <code>ENTRYPOINT</code> and <code>CMD</code> arguments in your Docker image with the <code>command</code> and <code>args</code> settings in the .yaml file.\n",
    "\n",
    "Start the training by going by entering the following command:\n",
    "```bash\n",
    "kubectl apply -f dialogue_train.yaml\n",
    "```\n",
    "Check that the pods statuses and ensure that they are starting or running by running <code>kubectl get pods</code>. If your code prints or displays the training progress, you can check on the progress via:\n",
    "```bash\n",
    "kubectl logs my-deep-learning-training-kube-1\n",
    "```\n",
    "<img src=\"dialogue_train_screenshot.JPG\"></img>\n",
    "\n",
    "You can also check whether your training has completed as well:\n",
    "```bash\n",
    "kubectl get pods\n",
    "```\n",
    "<img src=\"Kubernetes Get Pods.JPG\"></img>\n",
    "If Kubernetes is still running your training algorithm, the <code>STATUS</code> will display <code>RUNNING</code> instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details for Advanced Users\n",
    "\n",
    "The earlier section was a basic introduction into how to deploy your deep learning algorithm into the Kubernetes cluster. However, there are intrincacies into how your training code should run while in the cluster. This is done in our example to highlight some of the best practises within the industry.\n",
    "\n",
    "#### Loading the Data from a Central Storage\n",
    "\n",
    "Firstly, the Docker image size could potentially increase exponentially if the <code>dockerfile</code> includes the training dataset within the image built. To mitigate this, only codes (including cloning of code repositories via <code>git clone</code>) should be packaged into the Docker image. We recommend to to store your training data in the central S3 storage or the Hadoop cluster since these components were designed to store large amounts of data.\n",
    "\n",
    "Let us look at how to load our data from an S3 bucket. We use the <code>boto3</code> python package to connect to the S3 bucket.\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Load the file from the S3 bucket. #\n",
    "s3_client = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://minio.dsta.ai:9000' ,\n",
    "    aws_access_key_id='your_user_name',\n",
    "    aws_secret_access_key='your_password')\n",
    "s3_bucket = s3_client.Bucket('your_bucket_name')\n",
    "\n",
    "for obj in s3_bucket.objects.all():\n",
    "    key = obj.key\n",
    "    \n",
    "    # Download the data. #\n",
    "    if key == \"data/movie_dialogue.pkl\":\n",
    "        tmp_pkl = obj.get()[\"Body\"].read()\n",
    "        data_tuple, idx2word, word2idx = pkl.loads(tmp_pkl)\n",
    "    \n",
    "    # Download all codes. #\n",
    "    if key.find(\"codes/\") != -1 and key.endswith(\".py\"):\n",
    "        local_file_name = key.replace(\"codes/\", \"\")\n",
    "        s3_bucket.download_file(key, local_file_name)\n",
    "```\n",
    "\n",
    "In our example, we also wrote our custom python script (<code>tf_ver2_transformer.py</code>) to implement a slightly modified version of the <a href=https://arxiv.org/abs/1706.03762>Transformer</a> (a final residual connection connecting the embedding inputs to the outputs is applied). Using <code>s3_bucket.download_file(key, local_file_name)</code>, this script (and all other python scripts in the S3 bucket) is downloaded to the current working directory and directly imported into the main python program. The pickle file of the data <code>movie_dialogue.pkl\"</code> is also read into memory using <code>obj.get()[\"Body\"].read()</code>.\n",
    "\n",
    "When your training runs for a long time, it is desired to save your model parameters periodically. The trained model can then be downloaded directly from the MINIO interface and deployed to another machine if desired. This is useful, for example, when there is a need to deploy a trained model on-site without access to the AI platform. \n",
    "```python\n",
    "# Save the model to S3 bucket. #\n",
    "if n_iter % save_s3_step == 0:\n",
    "    model_files = [x[2] for x in os.walk(model_ckpt_dir)][0]\n",
    "    for model_file in model_files:\n",
    "        tmp_bucket_object = model_ckpt_dir + \"/\" + model_file\n",
    "        with open(tmp_bucket_object, \"rb\") as tmp_model_file:\n",
    "            s3_bucket.Object(tmp_bucket_object).put(Body=tmp_model_file)\n",
    "```\n",
    "Moving forward, we will be working on an approach to allow us to specify which training program to run as an argument to minimise building multiple Docker images which call the same building blocks. Once done, this article will be updated accordingly. \n",
    "\n",
    "That's it! We hope that you enjoyed reading this article :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix A:\n",
    "The python is here.  \n",
    "```python\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from nltk import wordpunct_tokenize as word_tokenizer\n",
    "\n",
    "# Define the weight update step. #\n",
    "#@tf.function\n",
    "def train_step(\n",
    "    model, x_encode, x_decode, x_output, \n",
    "    optimizer, learning_rate=1.0e-3, grad_clip=1.0):\n",
    "    optimizer.lr.assign(learning_rate)\n",
    "    \n",
    "    with tf.GradientTape() as grad_tape:\n",
    "        output_logits = model(x_encode, x_decode)\n",
    "        \n",
    "        tmp_losses = tf.reduce_mean(tf.reduce_sum(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=x_output, logits=output_logits), axis=1))\n",
    "    \n",
    "    tmp_gradients = \\\n",
    "        grad_tape.gradient(tmp_losses, model.trainable_variables)\n",
    "    \n",
    "    clipped_gradients, _ = \\\n",
    "        tf.clip_by_global_norm(tmp_gradients, grad_clip)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(clipped_gradients, model.trainable_variables))\n",
    "    return tmp_losses\n",
    "\n",
    "# Parse the arguments. #\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"-b\", \"--batch\", help=\"Batch size\", required=True)\n",
    "parser.add_argument(\n",
    "    \"-d\", \"--display\", help=\"Display steps\", required=True)\n",
    "parser.add_argument(\n",
    "    \"-n\", \"--n_iterations\", help=\"Number of iterations\", required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(\"Batch Size:\", str(args.batch))\n",
    "print(\"Display steps:\", str(args.display))\n",
    "print(\"No. of iterations:\", str(args.n_iterations))\n",
    "\n",
    "# Load the file from the S3 bucket. #\n",
    "s3_client = boto3.resource(\n",
    "    's3',\n",
    "    endpoint_url='http://minio.dsta.ai:9000' ,\n",
    "    aws_access_key_id='your_user_name',\n",
    "    aws_secret_access_key='your_password')\n",
    "s3_bucket = s3_client.Bucket('your_S3_bucket')\n",
    "\n",
    "for obj in s3_bucket.objects.all():\n",
    "    key = obj.key\n",
    "    \n",
    "    # Download the data. #\n",
    "    if key == \"data/movie_dialogue.pkl\":\n",
    "        tmp_pkl = obj.get()[\"Body\"].read()\n",
    "        data_tuple, idx2word, word2idx = pkl.loads(tmp_pkl)\n",
    "    \n",
    "    # Download all codes. #\n",
    "    if key.find(\"codes/\") != -1 and key.endswith(\".py\"):\n",
    "        local_file_name = key.replace(\"codes/\", \"\")\n",
    "        s3_bucket.download_file(key, local_file_name)\n",
    "\n",
    "# The custom module can only be imported after downloading the   #\n",
    "# python scripts from the Minio server to the current directory. #\n",
    "import tf_ver2_transformer as tf_transformer\n",
    "\n",
    "num_data  = len(data_tuple)\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary Size:\", str(vocab_size))\n",
    "\n",
    "SOS_token = word2idx[\"SOS\"]\n",
    "EOS_token = word2idx[\"EOS\"]\n",
    "PAD_token = word2idx[\"PAD\"]\n",
    "UNK_token = word2idx[\"UNK\"]\n",
    "\n",
    "# Model Parameters. #\n",
    "batch_size = int(args.batch)\n",
    "seq_encode = 15\n",
    "seq_decode = 16\n",
    "\n",
    "num_layers  = 6\n",
    "num_heads   = 16\n",
    "prob_keep   = 0.9\n",
    "hidden_size = 1024\n",
    "ffwd_size   = 4*hidden_size\n",
    "\n",
    "initial_lr    = 0.001\n",
    "gradient_clip = 1.00\n",
    "maximum_iter  = int(args.n_iterations)\n",
    "restore_flag  = False\n",
    "display_step  = int(args.display)\n",
    "save_s3_step  = 10000\n",
    "cooling_step  = 1000\n",
    "warmup_steps  = 2500\n",
    "anneal_step   = 2000\n",
    "anneal_rate   = 0.75\n",
    "\n",
    "model_ckpt_dir  = \\\n",
    "    \"TF_Models/transformer_seq2seq\"\n",
    "train_loss_file = \"dialogue_train_loss_transformer.csv\"\n",
    "\n",
    "# Set the number of threads to use. #\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "\n",
    "print(\"Building the Transformer Model.\")\n",
    "start_time = time.time()\n",
    "\n",
    "seq2seq_model = tf_transformer.TransformerNetwork(\n",
    "    num_layers, num_heads, hidden_size, ffwd_size, \n",
    "    vocab_size, vocab_size, seq_encode, seq_decode, \n",
    "    embed_size=hidden_size, p_keep=prob_keep)\n",
    "seq2seq_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(\"Transformer Model built (\" + str(elapsed_time) + \" mins).\")\n",
    "\n",
    "# Create the model checkpoint. #\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    step=tf.Variable(0), \n",
    "    seq2seq_model=seq2seq_model, \n",
    "    seq2seq_optimizer=seq2seq_optimizer)\n",
    "\n",
    "manager = tf.train.CheckpointManager(\n",
    "    ckpt, model_ckpt_dir, max_to_keep=1)\n",
    "\n",
    "if restore_flag:\n",
    "    # Download all model outputs. #\n",
    "    for obj in s3_bucket.objects.all():\n",
    "        key = obj.key\n",
    "        if key.find(model_ckpt_dir) != -1:\n",
    "            s3_bucket.download_file(key, key)\n",
    "    \n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Model restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Error: No latest checkpoint found.\")\n",
    "    \n",
    "    train_loss_df = pd.read_csv(train_loss_file)\n",
    "    train_loss_list = [tuple(\n",
    "        train_loss_df.iloc[x].values) for x in range(len(train_loss_df))]\n",
    "else:\n",
    "    print(\"Training a new model.\")\n",
    "    train_loss_list = []\n",
    "\n",
    "# Placeholders to store the batch data. #\n",
    "tmp_input   = np.zeros([batch_size, seq_encode], dtype=np.int32)\n",
    "tmp_seq_out = np.zeros([batch_size, seq_decode+1], dtype=np.int32)\n",
    "tmp_test_in = np.zeros([1, seq_encode], dtype=np.int32)\n",
    "tmp_test_dec = SOS_token * np.ones([1, seq_decode], dtype=np.int32)\n",
    "\n",
    "n_iter = ckpt.step.numpy().astype(np.int32)\n",
    "print(\"-\" * 50)\n",
    "print(\"Training the Transformer Network\", \n",
    "      \"(\" + str(n_iter), \"iterations).\")\n",
    "    \n",
    "tot_loss = 0.0\n",
    "start_tm = time.time()\n",
    "while n_iter < maximum_iter:\n",
    "    step_val = float(max(n_iter+1, warmup_steps))**(-0.5)\n",
    "    learn_rate_val = float(hidden_size)**(-0.5) * step_val\n",
    "    \n",
    "    batch_sample = np.random.choice(\n",
    "        num_data, size=batch_size, replace=False)\n",
    "    \n",
    "    tmp_input[:, :]   = PAD_token\n",
    "    tmp_seq_out[:, :] = PAD_token\n",
    "    tmp_seq_out[:, 0] = SOS_token\n",
    "    for n_index in range(batch_size):\n",
    "        tmp_index = batch_sample[n_index]\n",
    "        tmp_i_tok = data_tuple[tmp_index][0].split(\" \")\n",
    "        tmp_o_tok = data_tuple[tmp_index][1].split(\" \")\n",
    "\n",
    "        tmp_i_idx = [word2idx.get(x, UNK_token) for x in tmp_i_tok]\n",
    "        tmp_o_idx = [word2idx.get(x, UNK_token) for x in tmp_o_tok]\n",
    "        \n",
    "        n_input  = len(tmp_i_idx)\n",
    "        n_output = len(tmp_o_idx)\n",
    "        n_decode = n_output + 1\n",
    "\n",
    "        tmp_input[n_index, :n_input] = tmp_i_idx\n",
    "        tmp_seq_out[n_index, 1:n_decode] = tmp_o_idx\n",
    "        tmp_seq_out[n_index, n_decode] = EOS_token\n",
    "\n",
    "    tmp_decode = tmp_seq_out[:, :-1]\n",
    "    tmp_output = tmp_seq_out[:, 1:]\n",
    "    \n",
    "    tmp_loss = train_step(\n",
    "        seq2seq_model, tmp_input, tmp_decode, tmp_output, \n",
    "        seq2seq_optimizer, learning_rate=learn_rate_val)\n",
    "    \n",
    "    n_iter += 1\n",
    "    tot_loss += tmp_loss.numpy()\n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    if n_iter % display_step == 0:\n",
    "        end_time = time.time()\n",
    "        avg_loss = tot_loss / display_step\n",
    "        tot_loss = 0.0\n",
    "        elapsed_tm = (end_time - start_tm) / 60\n",
    "        start_tm   = time.time()\n",
    "\n",
    "        tmp_test_in[:, :] = PAD_token\n",
    "        sample_id = np.random.choice(num_data, size=1)\n",
    "        tmp_data  = data_tuple[sample_id[0]]\n",
    "\n",
    "        tmp_i_tok = tmp_data[0].split(\" \")\n",
    "        tmp_o_tok = tmp_data[1].split(\" \")\n",
    "        tmp_i_idx = [word2idx.get(x, UNK_token) for x in tmp_i_tok]\n",
    "\n",
    "        n_input = len(tmp_i_idx)\n",
    "        tmp_test_in[0, :n_input] = tmp_i_idx\n",
    "        \n",
    "        gen_ids = seq2seq_model.infer(tmp_test_in, tmp_test_dec)\n",
    "        gen_phrase = [idx2word[x] for x in gen_ids.numpy()[0]]\n",
    "        gen_phrase = \" \".join(gen_phrase)\n",
    "\n",
    "        print(\"Iteration\", str(n_iter) + \":\")\n",
    "        print(\"Elapsed Time:\", str(elapsed_tm) + \" mins.\")\n",
    "        print(\"Batch Size:\", str(batch_size) + \".\")\n",
    "        print(\"Average Loss:\", str(avg_loss))\n",
    "        print(\"Gradient Clip:\", str(gradient_clip))\n",
    "        print(\"Learning Rate:\", str(seq2seq_optimizer.lr.numpy()))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Input Phrase:\")\n",
    "        print(\" \".join([idx2word[x] for x in tmp_i_idx]))\n",
    "        print(\"Generated Phrase:\")\n",
    "        print(gen_phrase)\n",
    "        print(\"Actual Response:\")\n",
    "        print(tmp_data[1])\n",
    "        print(\"\")\n",
    "        \n",
    "        # Save the training progress. #\n",
    "        train_loss_list.append((n_iter, avg_loss))\n",
    "        train_loss_df = pd.DataFrame(\n",
    "            train_loss_list, columns=[\"n_iter\", \"xent_loss\"])\n",
    "        train_loss_df.to_csv(train_loss_file, index=False)\n",
    "        s3_bucket.Object(\"data/dialogue_train_loss.csv\").put(Body=train_loss_file)\n",
    "        \n",
    "        # Save the model. #\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved model to {}\".format(save_path))\n",
    "        \n",
    "        # Save the model to S3 bucket. #\n",
    "        if n_iter % save_s3_step == 0:\n",
    "            model_files = [x[2] for x in os.walk(model_ckpt_dir)][0]\n",
    "            for model_file in model_files:\n",
    "                tmp_bucket_object = model_ckpt_dir + \"/\" + model_file\n",
    "                with open(tmp_bucket_object, \"rb\") as tmp_model_file:\n",
    "                    s3_bucket.Object(tmp_bucket_object).put(Body=tmp_model_file)\n",
    "        print(\"-\" * 50)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
